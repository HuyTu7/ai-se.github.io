---
layout: page
title: Crazed idea.... Keys West
author: Tim Menzies
excerpt: Data mining meets MOEA/D 
---

Been reading on
[MOEA/D](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.320.1222&rank=1)
(see also
[PADE](http://pagmo.sourceforge.net/pygmo/documentation/algorithms.html#PyGMO.algorithm.pade). Its
kind of a meta-learner. It builds _islands_ then
runs a standard learner on each
island. E.g. MOEA/D-DE would run differential
evolution on various islands.

There are some standard methods for making the
islands but I was thinking, why not just use linear-time binary-split
[FastMap](https://github.com/ai-se/where/blob/master/where2.py#L26)?

Then, build recommendations for jumping from
_current_ to _better_, as follows.
For each current island _I1_..

+ Find ``better'' islands where ``better''
  means that for at least one objective,
  the [Cliff's delta effect size](https://github.com/ai-se/where/blob/5428935fd8138ed83c2bef2800d2b525bb80e949/cliff.py)  says they are truly
	   different and the medians are skewed in a ``better'' way.
+ For each island _I2_ build a contrast learning task as follows
  where class1= _I1_, class2= _I2_ and class3= every other island.
+ Discretize all numerics by
  [minimizing entropy of class1,class2,class3](https://github.com/timm/axe/blob/master/old/ediv.py).
+ Sort the ranges by
  BORE where  best=class2 and rest=class1,class3_ (for notes on BORE,
  see [section 4.2  of this paper](http://menzies.us/pdf/07casease.pdf).
+ Let the _value_ of the first _i_ items of that sort be what percentage
  of _class1,class2,class3_ instances that have those _i_ ranges
  contain _class2_ (the target class).
+ Return the smallest _i_ ranges where _i+1_ has less value.

If this is data mining (where no new data can be generated) then stop.
Call what you have ``islands, first generation''.
Else:

+ For each island with a contrast set, collect new instances by
  interpolating instances in that island, then applying the contrast set.
+ Repeat till new improvements are only epsilon better than last. This generates, ``islands,
  generation last''.
+ Run the above _current_ to _better_ algorithm using a combination of the first and last generation
  algorithms.

Not some short cuts:

+ Instead of discretizing for each new pair of current,better, discretize ONCE across all the islands. Proably would
  work just fine.
+ Once the data is discretized, build a reverse index from the ranges back to the candidates they select for. Which would make   testing the _value_ stuff very fast.  
+ When looking for better, be simpler. 
+ Active learning: on the way down with fastmap, prune dull islands. Also, when testing if one island is better than another,   only pick some items at random in each island (say, the small _m_ examples nearest the fastmap poles of each island)

But why is it called Keys West? An algorithm that builds bridges between islands?
That extends an older algorithm of mine called [Keys2](http://goo.gl/ucwCfV)?
Well, [see if you can figure that out](http://goo.gl/fG1pWx).

